{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/looptest/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torchvision import utils\n",
    "from model_drum import Generator\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('./melgan')\n",
    "from modules import Generator_melgan\n",
    "import yaml\n",
    "import os, random\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "from vscode_audio import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LATENT = 512\n",
    "N_MLP = 8\n",
    "SIZE_OUTPUT = 64 # size of output image\n",
    "\n",
    "#CHECKPOINT = \"./looperman_one_bar_checkpoint.pt\"\n",
    "CHECKPOINT = \"./freesound_checkpoint.pt\"\n",
    "\n",
    "DATAPATH = \"./data/looperman/\"\n",
    "\n",
    "TRUNCATION = 1\n",
    "TRUNCATION_MEAN = 4096\n",
    "\n",
    "SR = 44100\n",
    "\n",
    "device_name = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generator = Generator(SIZE_OUTPUT, N_LATENT, N_MLP, channel_multiplier=2).to(device_name)\n",
    "checkpoint = torch.load(CHECKPOINT, map_location=torch.device('cpu'))\n",
    "\n",
    "generator.load_state_dict(checkpoint[\"g_ema\"], strict=False)\n",
    "\n",
    "\n",
    "if TRUNCATION < 1:\n",
    "    with torch.no_grad():\n",
    "        mean_latent = generator.mean_latent(TRUNCATION_MEAN)\n",
    "else:\n",
    "    mean_latent = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocoder(device_name):\n",
    "    feat_dim = 80\n",
    "    mean_fp = f'{DATAPATH}/mean.mel.npy'\n",
    "    std_fp = f'{DATAPATH}/std.mel.npy'\n",
    "    v_mean = torch.from_numpy(np.load(mean_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    v_std = torch.from_numpy(np.load(std_fp)).float().view(1, feat_dim, 1).to(device_name)\n",
    "    vocoder_config_fp = './melgan/args.yml'\n",
    "    vocoder_config = read_yaml(vocoder_config_fp)\n",
    "\n",
    "    n_mel_channels = vocoder_config.n_mel_channels\n",
    "    ngf = vocoder_config.ngf\n",
    "    n_residual_layers = vocoder_config.n_residual_layers\n",
    "\n",
    "    vocoder = Generator_melgan(n_mel_channels, ngf, n_residual_layers).to(device_name)\n",
    "    vocoder.eval()\n",
    "\n",
    "    vocoder_param_fp = os.path.join('./melgan', 'best_netG.pt')\n",
    "    vocoder.load_state_dict(torch.load(vocoder_param_fp, map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "    return vocoder, v_mean, v_std\n",
    "VOCODER, V_MEAN, V_STD = load_vocoder(device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocode(sample, vocoder=VOCODER, v_mean=V_MEAN, v_std=V_STD):\n",
    "    de_norm = sample.squeeze(0) * v_std + v_mean\n",
    "    audio_output = vocoder(de_norm)\n",
    "    return audio_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "prev_sample_z = None\n",
    "\n",
    "def generate(g_ema, device, mean_latent, ckpt_name, sample_z = None, truncation=TRUNCATION, prev_coef=0.0):\n",
    "    global prev_sample_z\n",
    "    #epoch = ckpt_name.split('.')[0]\n",
    "    # os.makedirs(f'./tmp/{epoch}', exist_ok=True)\n",
    "    # os.makedirs(f'./tmp/{epoch}/mel_80_320', exist_ok=True)\n",
    "    n_samples = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        g_ema.eval()\n",
    "#        for i in tqdm(range(args.pics)):\n",
    "        if sample_z is None:\n",
    "            if prev_sample_z is None or prev_coef == 0.0:\n",
    "                sample_z = torch.randn(1, N_LATENT, device=device)\n",
    "                sample_z = sample_z.repeat(n_samples, 1)\n",
    "            else:\n",
    "                sample_z = prev_sample_z + torch.randn(n_samples, N_LATENT, device=device) * prev_coef\n",
    "\n",
    "        prev_sample_z = sample_z\n",
    "  \n",
    "        sample, _ = g_ema([sample_z], truncation=truncation, truncation_latent=mean_latent)\n",
    "      \n",
    "#        np.save(f'./tmp/{epoch}/mel_80_320/{i}.npy', sample.squeeze().data.cpu().numpy())\n",
    "#        print(sample)\n",
    "\n",
    "        randid = random.randint(0, 10000)\n",
    "        imagepath = f'/tmp/img_{randid}.png'\n",
    "        utils.save_image(sample, imagepath, nrow=1, normalize=True, value_range=(-1, 1))\n",
    "    \n",
    "        # for i in range(n_samples):\n",
    "        channels = []\n",
    "        filepath = f'/tmp/gem_{randid}.wav'\n",
    "        for i in range(n_samples):\n",
    "            audio_output = vocode(sample[i])\n",
    "            audio_output = audio_output.squeeze().detach().cpu().numpy() \n",
    "            print(audio_output.shape, audio_output.dtype)\n",
    "            channel = AudioSegment( (audio_output*np.iinfo(np.int16).max).astype(\"int16\").tobytes(), sample_width=2, # 16 bit \n",
    "                    frame_rate=SR, channels=1)\n",
    "            channels.append(channel)\n",
    "\n",
    "            filepath_ = f'/tmp/gem_{randid}_{i}.wav'\n",
    "            sf.write(filepath_, audio_output, SR)\n",
    "\n",
    "        multich = AudioSegment.from_mono_audiosegments(*channels)\n",
    "        multich.export(filepath, format=\"wav\")\n",
    "        # outputs = torch.vstack(outputs)\n",
    "        # print(sample.shape, outputs.shape)\n",
    "#            filepath = f'/tmp/gem_{randid}_{i}.wav'\n",
    "#            sf.write(filepath, audio_output.squeeze().detach().cpu().numpy(), SR)\n",
    "#            filepaths.append(filepath)\n",
    "        return filepath, imagepath\n",
    "            # sf.write(f'{args.store_path}/{epoch}/{i}.wav', audio_output.squeeze().detach().cpu().numpy(), sr)\n",
    "            # print('generate {}th wav file'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81920,) float32\n",
      "(81920,) float32\n",
      "(81920,) float32\n",
      "(81920,) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tmp/gem_3139.wav', '/tmp/img_3139.png')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #generate(generator, device_name, mean_latent, CHECKPOINT, sample_z=None, prev_coef=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# z_coord = torch.randn(4, N_LATENT, device=device_name)\n",
    "\n",
    "# def coord_to_z(x, y):\n",
    "#     assert x >= 0 and x <= 1\n",
    "#     assert y >= 0 and y <= 1\n",
    "\n",
    "#     z =  ((1-math.sqrt(x**2 + y**2)) * z_coord[0] + (1 - math.sqrt((1-x)**2+y**2)) * z_coord[1] + \\\n",
    "#     (1 - math.sqrt(x**2 + (1-y)**2)) * z_coord[2] + (1 - math.sqrt((1-x)**2+(1-y)**2)) * z_coord[3]) \n",
    "#     z = torch.unsqueeze(z, 0)\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_z = coord_to_z(0, 0)\n",
    "# print(sample_z[:10], z_coord[0][:10])\n",
    "# sample_z = coord_to_z(0, 1)\n",
    "# print(sample_z[:10], z_coord[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample_z = coord_to_z(0, 0.5)\n",
    "# audio_output = generate(generator, device_name, mean_latent, CHECKPOINT, sample_z=sample_z)\n",
    "# audio_output.shape\n",
    "\n",
    "# Audio(audio_output, sr=SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vscode_audio import *\n",
    "\n",
    "# Audio(audio_output, sr=SR)\n",
    "\n",
    "# Audio(audio_output, sr=SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving on ('127.0.0.1', 10015)\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/looptest/lib/python3.8/site-packages/torchvision/utils.py:63: UserWarning: The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. Please use 'value_range' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "org torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server, udp_client\n",
    "import os, random\n",
    "\n",
    "client = udp_client.SimpleUDPClient('127.0.0.1', 10018)\n",
    "\n",
    "def generate_random(unused_addr, prev_coef):\n",
    "    # try:\n",
    "    audiopath, imagepath = generate(generator, device_name, mean_latent, CHECKPOINT, sample_z=None, prev_coef=prev_coef) # random sample\n",
    "    client.send_message(\"/generated\", (audiopath, imagepath))\n",
    "    #except Exception as exp:\n",
    "    # print(\"Error in /find_loops\", exp)        \n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/generate\", generate_random)\n",
    "\n",
    "server = osc_server.ThreadingOSCUDPServer(\n",
    "    ('localhost', 10015), dispatcher)\n",
    "print(\"Serving on {}\".format(server.server_address))\n",
    "server.serve_forever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70899268a59b9e14302f4ecef5a4c87b2b8a3484e36d1ae41be0a1619eb013cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('looptest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
